{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess import base_data\n",
    "from pathlib import Path\n",
    "import hydra\n",
    "import yaml\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import catboost as cb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert coordinates from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../config/config.yaml'\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    yaml = yaml.safe_load(file)\n",
    "\n",
    "# featuresリストを取得\n",
    "features = yaml.get('features', [])\n",
    "\n",
    "data = [pd.read_pickle(f\"../features/{f}.pkl\") for f in features]\n",
    "data = pd.concat(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spring'] = data['ApprovalMonth'].isin([3, 4, 5]).astype(int)\n",
    "data['Summer'] = data['ApprovalMonth'].isin([6, 7, 8]).astype(int)\n",
    "data['Autumn'] = data['ApprovalMonth'].isin([9, 10, 11]).astype(int)\n",
    "data['Winter'] = data['ApprovalMonth'].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "\n",
    "# Calculate the count of urban and rural businesses in each state\n",
    "urban_rural_counts = data.groupby(['State', 'UrbanRural']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the ratio of urban to rural businesses in each state\n",
    "urban_rural_counts['UrbanRuralBusinessRatio'] = urban_rural_counts[1] / (urban_rural_counts[0] + urban_rural_counts[1])\n",
    "urban_rural_counts = urban_rural_counts['UrbanRuralBusinessRatio']\n",
    "\n",
    "# Merge this ratio back into the original dataframe\n",
    "data = data.merge(urban_rural_counts, on='State', how='left')\n",
    "\n",
    "# Handle possible division by zero\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 各行の欠損値の数を計算\n",
    "data['missing_values_count'] = data.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_coords = {\n",
    "    'AL': (32.806671, -86.791130), 'AK': (61.370716, -152.404419), 'AZ': (33.729759, -111.431221),\n",
    "    'AR': (34.969704, -92.373123), 'CA': (36.116203, -119.681564), 'CO': (39.059811, -105.311104),\n",
    "    'CT': (41.597782, -72.755371), 'DE': (39.318523, -75.507141), 'FL': (27.766279, -81.686783),\n",
    "    'GA': (33.040619, -83.643074), 'HI': (21.094318, -157.498337), 'ID': (44.240459, -114.478828),\n",
    "    'IL': (40.349457, -88.986137), 'IN': (39.849426, -86.258278), 'IA': (42.011539, -93.210526),\n",
    "    'KS': (38.526600, -96.726486), 'KY': (37.668140, -84.670067), 'LA': (31.169546, -91.867805),\n",
    "    'ME': (44.693947, -69.381927), 'MD': (39.063946, -76.802101), 'MA': (42.230171, -71.530106),\n",
    "    'MI': (43.326618, -84.536095), 'MN': (45.694454, -93.900192), 'MS': (32.741646, -89.678696),\n",
    "    'MO': (38.456085, -92.288368), 'MT': (46.921925, -110.454353), 'NE': (41.125370, -98.268082),\n",
    "    'NV': (38.313515, -117.055374), 'NH': (43.452492, -71.563896), 'NJ': (40.298904, -74.521011),\n",
    "    'NM': (34.840515, -106.248482), 'NY': (42.165726, -74.948051), 'NC': (35.630066, -79.806419),\n",
    "    'ND': (47.528912, -99.784012), 'OH': (40.388783, -82.764915), 'OK': (35.565342, -96.928917),\n",
    "    'OR': (44.572021, -122.070938), 'PA': (40.590752, -77.209755), 'RI': (41.680893, -71.511780),\n",
    "    'SC': (33.856892, -80.945007), 'SD': (44.299782, -99.438828), 'TN': (35.747845, -86.692345),\n",
    "    'TX': (31.054487, -97.563461), 'UT': (40.150032, -111.862434), 'VT': (44.045876, -72.710686),\n",
    "    'VA': (37.769337, -78.169968), 'WA': (47.400902, -121.490494), 'WV': (38.491226, -80.954570),\n",
    "    'WI': (44.268543, -89.616508), 'WY': (42.755966, -107.302490)\n",
    "}\n",
    "\n",
    "# Apply the mapping to the dataset to create new latitude and longitude columns\n",
    "data['Latitude'] = data['State'].map(lambda x: state_to_coords[x][0] if x in state_to_coords else None)\n",
    "data['Longitude'] = data['State'].map(lambda x: state_to_coords[x][1] if x in state_to_coords else None)\n",
    "data['BankState_Latitude'] = data['BankState'].map(lambda x: state_to_coords[x][0] if x in state_to_coords else None)\n",
    "data['BankState_Longitude'] = data['BankState'].map(lambda x: state_to_coords[x][1] if x in state_to_coords else None)\n",
    "\n",
    "# Calculate the distance between State and BankState\n",
    "data['State_BankState_Distance'] = data.apply(lambda x: haversine(x['Latitude'], x['Longitude'], \n",
    "                                                                   x['BankState_Latitude'], x['BankState_Longitude']) \n",
    "                                              if pd.notnull(x['Latitude']) and pd.notnull(x['BankState_Latitude']) \n",
    "                                              else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"train\"]==True].drop(columns=\"train\")\n",
    "test = data[data['train'] == False]\n",
    "test = test.drop(['train','MIS_Status'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['Term']]\n",
    "# スケーラーの初期化と適用\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# クラスタ数の候補を設定\n",
    "n_components = np.arange(1, 10)\n",
    "\n",
    "# BICを格納するためのリスト\n",
    "bics = []\n",
    "\n",
    "# 各クラスタ数に対してGMMをフィットし、BICを計算\n",
    "for n in n_components:\n",
    "    gmm = GaussianMixture(n_components=n, random_state=0)\n",
    "    gmm.fit(X)\n",
    "    bics.append(gmm.bic(X))\n",
    "\n",
    "X = pd.DataFrame(X,columns=[\"Term\"])\n",
    "\n",
    "# クラスタ数を9に設定し、GMMをフィット\n",
    "gmm = GaussianMixture(n_components=9, random_state=0)\n",
    "gmm.fit(X)\n",
    "\n",
    "# 各データポイントのクラスタ割り当てを取得\n",
    "cluster_assignments = gmm.predict(X)\n",
    "\n",
    "# クラスタ割り当て結果をデータフレームに追加\n",
    "X_clustered = X.copy()\n",
    "train['Term_Cluster'] = cluster_assignments\n",
    "\n",
    "# 'Term' 列の選択\n",
    "X_test = test[['Term']]\n",
    "\n",
    "# 訓練データでフィットさせたGMMを使用してクラスタ割り当てを予測\n",
    "test['Term_Cluster'] = gmm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = [\n",
    "    'State',\n",
    "    'BankState', \n",
    "    'Sector',  \n",
    "    'NewExist',\n",
    "    'UrbanRural',\n",
    "    'RevLineCr', \n",
    "    'LowDoc',\n",
    "    'FranchiseFlag',\n",
    "    'IsSameState',\n",
    "    'Term_Cluster',\n",
    "    'Spring',\n",
    "    'Summer',\n",
    "    'Autumn',\n",
    "    'Winter',\n",
    "    'BankStateCount',\n",
    "    'CityCount',\n",
    "    'SectorCount',\n",
    "    'StateCount',\n",
    "    'RealEstate',\n",
    "    'GreatRecession',\n",
    "    'AppvDisbursed',\n",
    "    \n",
    "    ]\n",
    "\n",
    "for col in category_features:\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('City',axis=1)\n",
    "test = test.drop('City',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# クロスバリデーションの設定\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# OOF予測用の空配列\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "\n",
    "# テストデータ予測用の空配列\n",
    "test_preds = np.zeros(test.shape[0])\n",
    "\n",
    "# クロスバリデーション\n",
    "for train_index, val_index in kf.split(train):\n",
    "    X_train, X_val = train.iloc[train_index], train.iloc[val_index]\n",
    "\n",
    "    # 目的変数の設定\n",
    "    y_train, y_val = X_train['MIS_Status'], X_val['MIS_Status']\n",
    "\n",
    "    X_train = X_train.drop(['MIS_Status'], axis=1)\n",
    "    X_val = X_val.drop(['MIS_Status'], axis=1)\n",
    "\n",
    "    # モデルの訓練\n",
    "    model = cb.CatBoostClassifier(verbose=False, early_stopping_rounds=100)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],cat_features=category_features)\n",
    "\n",
    "    # OOF予測\n",
    "    oof_preds[val_index] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    test_preds += model.predict_proba(test)[:, 1] / n_splits\n",
    "\n",
    "# 訓練データにOOF予測を追加\n",
    "train['catboost_preds'] = oof_preds\n",
    "\n",
    "# テストデータに予測を追加\n",
    "test['catboost_preds'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データとテストデータのCSVファイルに保存する列の選択\n",
    "train_csv = train[['ID', 'catboost_preds']]\n",
    "test_csv = test[['ID', 'catboost_preds']]\n",
    "\n",
    "# CSVファイルに保存\n",
    "train_csv_path = '../data/train_cb_predictions.csv'\n",
    "test_csv_path = '../data/test_cb_predictions.csv'\n",
    "\n",
    "train_csv.to_csv(train_csv_path, index=False)\n",
    "test_csv.to_csv(test_csv_path, index=False)\n",
    "\n",
    "train = train.drop(['ID', 'catboost_preds'],axis=1)\n",
    "test = test.drop(['ID', 'catboost_preds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('MIS_Status', axis=1)\n",
    "y_train = train['MIS_Status']\n",
    "\n",
    "# Further splitting the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostデータセットの作成\n",
    "train_pool = cb.Pool(X_train, y_train,cat_features=category_features)\n",
    "test_pool = cb.Pool(X_val, y_val,cat_features=category_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for seed in range(10):\n",
    "    # データサンプリング\n",
    "    train_sample = X_train.sample(frac=0.8, random_state=seed)\n",
    "    y_sample = y_train[train_sample.index]\n",
    "\n",
    "    # モデルの学習\n",
    "    cat_clf = cb.CatBoostClassifier()  # 進行状況を表示しない場合はverbose=0\n",
    "    cat_clf.fit(train_sample, y_sample,cat_features=category_features)\n",
    "\n",
    "    # 検証データに対する予測\n",
    "    y_pred = cat_clf.predict(X_val)\n",
    "    y_pred_proba = cat_clf.predict_proba(X_val)[:, 1]  # AUCの計算に使用\n",
    "\n",
    "    # モデルをリストに追加\n",
    "    models.append(cat_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. すべてのモデルを使用してトレーニングセットとテストセットの予測を行う。\n",
    "y_preds_train = []\n",
    "y_preds_test = []\n",
    "for model in models:\n",
    "    y_preds_train.append(model.predict_proba(X_train)[:, 1])\n",
    "    y_preds_test.append(model.predict_proba(X_val)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 予測結果を平均して、最終的な予測を得る。\n",
    "y_pred_train = np.mean(y_preds_train, axis=0)\n",
    "y_pred = np.mean(y_preds_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_val, y_pred))\n",
    "total_cases = len(y_val)  # テストデータの総数\n",
    "TP = (y_val == 1) & (y_pred >= 0.5)  # True positives\n",
    "FP = (y_val == 0) & (y_pred >= 0.5)  # False positives\n",
    "TN = (y_val == 0) & (y_pred < 0.5)  # True negatives\n",
    "FN = (y_val == 1) & (y_pred < 0.5)  # False negatives\n",
    "\n",
    "TP_count = sum(TP)\n",
    "FP_count = sum(FP)\n",
    "TN_count = sum(TN)\n",
    "FN_count = sum(FN)\n",
    "\n",
    "accuracy_TP = TP_count / total_cases * 100\n",
    "misclassification_rate_FP = FP_count / total_cases * 100\n",
    "accuracy_TN = TN_count / total_cases * 100\n",
    "misclassification_rate_FN = FN_count / total_cases * 100\n",
    "\n",
    "print(\"Total cases:\", total_cases)\n",
    "print(\"True positives:\", TP_count, \"(\", \"{:.2f}\".format(accuracy_TP), \"%)\")\n",
    "print(\"False positives:\", FP_count, \"(\", \"{:.2f}\".format(misclassification_rate_FP), \"%)\")\n",
    "print(\"True negatives:\", TN_count, \"(\", \"{:.2f}\".format(accuracy_TN), \"%)\")\n",
    "print(\"False negatives:\", FN_count, \"(\", \"{:.2f}\".format(misclassification_rate_FN), \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('accuracy_score, precision_score, recall_score, f1_score, roc_auc_score')\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)  # 0.5が閾値\n",
    "\n",
    "# スコア関数の修正\n",
    "score_funcs = [\n",
    "    accuracy_score, \n",
    "    lambda y_true, y_pred: precision_score(y_true, y_pred, average='binary'), \n",
    "    lambda y_true, y_pred: recall_score(y_true, y_pred, average='binary'),\n",
    "    lambda y_true, y_pred: f1_score(y_true, y_pred, average='binary'),\n",
    "    roc_auc_score  # この関数は確率をそのまま使用\n",
    "]\n",
    "\n",
    "# スコアの計算（roc_auc_score以外はy_pred_binaryを使用）\n",
    "scores = [\n",
    "    round(f(y_val, y_pred_binary if f != roc_auc_score else y_pred), 3)\n",
    "    for f in score_funcs\n",
    "]\n",
    "\n",
    "print(', '.join(map(str, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "#0.905, 0.915, 0.985, 0.949, 0.773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルからの予測確率を集める\n",
    "y_preds_proba = [model.predict_proba(test) for model in models]\n",
    "\n",
    "# 予測確率の平均を計算する\n",
    "y_proba_avg = np.mean(y_preds_proba, axis=0)\n",
    "\n",
    "# 最も高い確率のクラスを選択する\n",
    "y_new_pred = np.argmax(y_proba_avg, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0の割合: 10.50%\n",
      "1の割合: 89.50%\n"
     ]
    }
   ],
   "source": [
    "# 閾値を設定（例：0.5）\n",
    "threshold = 0.813\n",
    "\n",
    "# 閾値を超える確率を持つ予測を1とし、それ以外を0とする\n",
    "y_pred_threshold = (y_proba_avg[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# 0と1の割合を計算\n",
    "zeros = np.sum(y_pred_threshold == 0) / len(y_pred_threshold)\n",
    "ones = np.sum(y_pred_threshold == 1) / len(y_pred_threshold)\n",
    "\n",
    "# 結果を表示\n",
    "print(f\"0の割合: {zeros * 100:.2f}%\")\n",
    "print(f\"1の割合: {ones * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv', header=None)\n",
    "\n",
    "submit.iloc[:, 1] = y_pred_threshold\n",
    "filename = 'CB_base'\n",
    "\n",
    "submit.to_csv('outputs/' + filename + '.csv', index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の重要度を取得\n",
    "importance = cat_clf.feature_importances_\n",
    "\n",
    "# 特徴量の名前を取得\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# 特徴量の重要度を降順にソート\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# 特徴量の重要度を降順に表示\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, feature_names[indices[f]], importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FPR, TPR, およびしきい値を計算する\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "\n",
    "# AUC（Area Under Curve）を計算する\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC曲線をプロットする\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# 精度と再現率を計算\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "\n",
    "# PR AUCを計算\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# PR曲線をプロット\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
